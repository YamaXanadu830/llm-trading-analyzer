#!/usr/bin/env python3
"""
Twelve Data APIå®¢æˆ·ç«¯ - EURUSDå¤–æ±‡æ•°æ®è·å–
å…è´¹ç‰ˆé™åˆ¶: 8æ¬¡/åˆ†é’Ÿ, 800æ¬¡/å¤© - å®Œå…¨æ»¡è¶³éœ€æ±‚
"""

import requests
import pandas as pd
import sqlite3
import json
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
import os

def validate_date_format(date_string: str) -> bool:
    """éªŒè¯æ—¥æœŸæ ¼å¼ yyyy-MM-dd"""
    try:
        datetime.strptime(date_string, '%Y-%m-%d')
        return True
    except ValueError:
        return False

class TwelveDataClient:
    """Twelve Data APIå®¢æˆ·ç«¯"""
    
    BASE_URL = "https://api.twelvedata.com"
    
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆä¼˜å…ˆäºç¯å¢ƒå˜é‡ï¼‰ã€‚å¦‚æœªæä¾›ï¼Œå°†ä»ç¯å¢ƒå˜é‡è¯»å–ã€‚
        """
        self.session = requests.Session()
        
        # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼ä¼ å…¥çš„ api_keyï¼›å¦åˆ™å°è¯•ä»ç¯å¢ƒå˜é‡/.env.local åŠ è½½
        self.api_key = api_key or self._load_api_key_from_env()
        
        # å…è´¹ç‰ˆé™åˆ¶ç®¡ç†
        self.calls_per_minute = 8
        self.calls_per_day = 800
        self.call_times = []
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“ï¼ˆè·¯å¾„é”šå®šåˆ°æ¨¡å—ç›®å½• data/ï¼‰"""
        # å°†æ•°æ®ç›®å½•å›ºå®šä¸ºå½“å‰æ¨¡å—åŒçº§çš„ data/
        import os
        base_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(base_dir, "data")
        os.makedirs(data_dir, exist_ok=True)
        self.db_path = os.path.join(data_dir, "forex_data.db")

        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS price_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    datetime TEXT NOT NULL,
                    open REAL NOT NULL,
                    high REAL NOT NULL,
                    low REAL NOT NULL,
                    close REAL NOT NULL,
                    volume INTEGER,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(symbol, timeframe, datetime)
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_symbol_timeframe_datetime 
                ON price_data(symbol, timeframe, datetime)
            """)
    
    def _check_rate_limit(self):
        """æ£€æŸ¥APIè°ƒç”¨é¢‘ç‡é™åˆ¶"""
        now = datetime.now()
        
        # æ¸…ç†1åˆ†é’Ÿå‰çš„è°ƒç”¨è®°å½•
        self.call_times = [t for t in self.call_times if (now - t).seconds < 60]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ¯åˆ†é’Ÿé™åˆ¶
        if len(self.call_times) >= self.calls_per_minute:
            wait_time = 60 - (now - self.call_times[0]).seconds
            print(f"â³ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
            time.sleep(wait_time + 1)
            self.call_times = []
    
    def _make_request(self, endpoint: str, params: Dict) -> Dict:
        """å‘èµ·APIè¯·æ±‚"""
        self._check_rate_limit()
        
        if not self.api_key:
            raise RuntimeError("æœªé…ç½® TwelveData API Keyã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ 'TWELVEDATA_API_KEY' æˆ–åœ¨é¡¹ç›®ç›®å½•åˆ›å»º .env.local å¹¶é…ç½® TWELVEDATA_API_KEY=")
        
        params['apikey'] = self.api_key
        
        url = f"{self.BASE_URL}/{endpoint}"
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            self.call_times.append(datetime.now())
            
            data = response.json()
            
            # æ£€æŸ¥APIé”™è¯¯
            if 'status' in data and data['status'] == 'error':
                raise Exception(f"APIé”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
            return data
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
            raise

    def _load_api_key_from_env(self) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡æˆ– .env.local è¯»å– API Keyï¼ˆä¸æŠ¥é”™ï¼Œå»¶è¿Ÿåˆ°è¯·æ±‚é˜¶æ®µæ ¡éªŒï¼‰ã€‚"""
        import os
        api_key = os.getenv("TWELVEDATA_API_KEY") or os.getenv("TWELVE_DATA_API_KEY")
        if api_key:
            return api_key
        
        # å°è¯•åŠ è½½é¡¹ç›®å†… .env.local / .envï¼ˆè‹¥å®‰è£…äº† python-dotenvï¼‰
        try:
            from dotenv import load_dotenv
            project_root = os.path.dirname(os.path.abspath(__file__))
            # ä¼˜å…ˆ .env.localï¼Œç„¶å .env
            for env_name in [".env.local", ".env"]:
                env_path = os.path.join(os.path.dirname(project_root), env_name)
                if os.path.exists(env_path):
                    load_dotenv(env_path, override=False)
                    api_key = os.getenv("TWELVEDATA_API_KEY") or os.getenv("TWELVE_DATA_API_KEY")
                    if api_key:
                        return api_key
        except Exception:
            # ä¸å¼ºä¾èµ– python-dotenvï¼Œæ‰¾ä¸åˆ°å°±å¿½ç•¥ï¼Œäº¤ç”±è¯·æ±‚é˜¶æ®µæç¤º
            pass
        
        return None
    
    def get_forex_data(self, 
                      symbol: str = "EUR/USD", 
                      interval: str = "15min", 
                      outputsize: int = 2000,
                      start_date: str = None,
                      end_date: str = None) -> pd.DataFrame:
        """
        è·å–å¤–æ±‡Kçº¿æ•°æ®
        
        Args:
            symbol: è´§å¸å¯¹ï¼Œé»˜è®¤EUR/USD
            interval: æ—¶é—´å‘¨æœŸ (1min, 5min, 15min, 30min, 45min, 1h, 2h, 4h, 1day, 1week, 1month)
            outputsize: æ•°æ®æ•°é‡ï¼Œé»˜è®¤2000æ ¹Kçº¿
            start_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
        
        Returns:
            pandas.DataFrame: Kçº¿æ•°æ®
        """
        if start_date and end_date:
            print(f"ğŸ“Š è·å– {symbol} {interval} æ•°æ®ï¼Œæ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        else:
            print(f"ğŸ“Š è·å– {symbol} {interval} æ•°æ®ï¼Œæ•°é‡: {outputsize}")
        
        params = {
            'symbol': symbol,
            'interval': interval,
            'format': 'json'
        }
        
        # æ ¹æ®æ˜¯å¦æŒ‡å®šæ—¥æœŸèŒƒå›´é€‰æ‹©å‚æ•°
        if start_date and end_date:
            params['start_date'] = start_date
            params['end_date'] = end_date
        else:
            params['outputsize'] = outputsize
        
        try:
            data = self._make_request('time_series', params)
            
            if 'values' not in data:
                print(f"âŒ æ•°æ®æ ¼å¼é”™è¯¯: {data}")
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(data['values'])
            
            if df.empty:
                print("âŒ è·å–åˆ°ç©ºæ•°æ®")
                return df
            
            # æ•°æ®ç±»å‹è½¬æ¢
            df['datetime'] = pd.to_datetime(df['datetime'])
            for col in ['open', 'high', 'low', 'close']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            if 'volume' in df.columns:
                df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
            else:
                df['volume'] = 0
            
            # æŒ‰æ—¶é—´æ’åº
            df = df.sort_values('datetime').reset_index(drop=True)
            
            print(f"âœ… æˆåŠŸè·å– {len(df)} æ ¹Kçº¿æ•°æ®")
            return df
            
        except Exception as e:
            print(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def save_to_database(self, df: pd.DataFrame, symbol: str, timeframe: str):
        """ä¿å­˜æ•°æ®åˆ°SQLiteæ•°æ®åº“"""
        if df.empty:
            print("âŒ æ— æ•°æ®å¯ä¿å­˜")
            return
        
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“: {symbol} {timeframe}")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                for _, row in df.iterrows():
                    conn.execute("""
                        INSERT OR REPLACE INTO price_data 
                        (symbol, timeframe, datetime, open, high, low, close, volume)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        symbol, timeframe, 
                        row['datetime'].strftime('%Y-%m-%d %H:%M:%S'),
                        row['open'], row['high'], row['low'], row['close'],
                        row['volume'] if pd.notna(row['volume']) else 0
                    ))
                
                print(f"âœ… å·²ä¿å­˜ {len(df)} æ¡è®°å½•")
                
        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
    
    def load_from_database(self, symbol: str, timeframe: str, limit: int = 200) -> pd.DataFrame:
        """ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                query = """
                    SELECT datetime, open, high, low, close, volume
                    FROM price_data 
                    WHERE symbol = ? AND timeframe = ?
                    ORDER BY datetime DESC
                    LIMIT ?
                """
                df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
                
                if not df.empty:
                    df['datetime'] = pd.to_datetime(df['datetime'])
                    df = df.sort_values('datetime').reset_index(drop=True)
                    print(f"ğŸ“‚ ä»æ•°æ®åº“åŠ è½½ {len(df)} æ¡è®°å½•")
                
                return df
                
        except Exception as e:
            print(f"âŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_eurusd_data(self, use_cache: bool = True) -> pd.DataFrame:
        """
        è·å–EURUSD 15åˆ†é’Ÿæ•°æ® (ä¼˜åŒ–ç‰ˆ)
        
        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨æ•°æ®åº“ç¼“å­˜
        
        Returns:
            pandas.DataFrame: EURUSD Kçº¿æ•°æ®
        """
        symbol = "EUR/USD"
        timeframe = "15min"
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_data = self.load_from_database(symbol, timeframe)
            if not cached_data.empty:
                # æ£€æŸ¥æœ€æ–°æ•°æ®æ—¶é—´
                latest_time = cached_data['datetime'].max()
                time_diff = datetime.now() - latest_time.replace(tzinfo=None)
                
                # å¦‚æœæ•°æ®è¾ƒæ–°(< 1å°æ—¶)ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜
                if time_diff < timedelta(hours=1):
                    print(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œæœ€æ–°æ—¶é—´: {latest_time}")
                    return cached_data
        
        # ä»APIè·å–æ–°æ•°æ®
        df = self.get_forex_data(symbol, timeframe, 200)
        
        if not df.empty:
            self.save_to_database(df, symbol, timeframe)
        
        return df
    
    def get_multi_timeframe_data(self, 
                                symbol: str = "EUR/USD", 
                                timeframes: List[str] = None,
                                outputsize: int = 200,
                                use_cache: bool = True) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–å¤šä¸ªæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®
        
        Args:
            symbol: è´§å¸å¯¹
            timeframes: æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼Œé»˜è®¤["15min", "1h", "4h", "1day"]
            outputsize: æ¯ä¸ªæ—¶é—´å‘¨æœŸçš„æ•°æ®é‡
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
        Returns:
            Dict[str, pd.DataFrame]: æ—¶é—´å‘¨æœŸ->æ•°æ®çš„å­—å…¸
        """
        if timeframes is None:
            timeframes = ["15min", "1h", "4h", "1day"]
        
        print(f"ğŸ“Š æ‰¹é‡è·å– {symbol} å¤šæ—¶é—´å‘¨æœŸæ•°æ®: {timeframes}")
        result = {}
        
        for timeframe in timeframes:
            try:
                # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
                if use_cache:
                    cached_data = self.load_from_database(symbol, timeframe)
                    if not cached_data.empty:
                        latest_time = cached_data['datetime'].max()
                        time_diff = datetime.now() - latest_time.replace(tzinfo=None)
                        
                        # æ ¹æ®æ—¶é—´å‘¨æœŸè°ƒæ•´ç¼“å­˜æœ‰æ•ˆæœŸ
                        cache_hours = {"15min": 1, "1h": 2, "4h": 8, "1day": 24}.get(timeframe, 2)
                        if time_diff < timedelta(hours=cache_hours):
                            print(f"âœ… {timeframe} ä½¿ç”¨ç¼“å­˜æ•°æ®")
                            result[timeframe] = cached_data
                            continue
                
                # ä»APIè·å–æ–°æ•°æ®
                df = self.get_forex_data(symbol, timeframe, outputsize)
                if not df.empty:
                    self.save_to_database(df, symbol, timeframe)
                    result[timeframe] = df
                else:
                    print(f"âš ï¸ {timeframe} è·å–æ•°æ®å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ {timeframe} å¤„ç†å¤±è´¥: {e}")
        
        print(f"âœ… æˆåŠŸè·å– {len(result)} ä¸ªæ—¶é—´å‘¨æœŸçš„æ•°æ®")
        return result

    def download_last_year_in_chunks(self,
                                     symbol: str = "EUR/USD",
                                     timeframe: str = "1min",
                                     chunk_days: float = 3.5) -> Dict[str, Any]:
        """
        ä¸‹è½½æœ€è¿‘ä¸€å¹´çš„æ•°æ®ï¼ˆé»˜è®¤1minï¼‰ï¼ŒæŒ‰çª—å£åˆ†æ®µå…¥åº“ï¼Œçª—å£è·¨åº¦çº¦ç­‰äº5000æ ¹/æ¬¡ã€‚

        è¯´æ˜ï¼š
        - 1min å‘¨æœŸçº¦ 1 å¤© â‰ˆ 1440 æ ¹ï¼Œ5000 æ ¹â‰ˆ 3.47 å¤©ï¼›é»˜è®¤ chunk_days=3.5 è´´è¿‘ 5000 æ ¹/æ‰¹ã€‚
        - ä½¿ç”¨æ—¥æœŸçª—å£ï¼ˆYYYY-MM-DDï¼‰è¿›è¡Œåˆ†æ®µï¼ŒTwelveData å°†è¿”å›åŒºé—´å†…æ‰€æœ‰å¯ç”¨æ•°æ®ã€‚
        - å­˜åœ¨é‡å¤æ•°æ®æ—¶ç”±æ•°æ®åº“ UNIQUE(symbol,timeframe,datetime) å»é‡ã€‚
        """
        print("\nğŸš€ åˆ†æ®µä¸‹è½½æœ€è¿‘ä¸€å¹´çš„æ•°æ®")
        print(f"ğŸ•’ å‘¨æœŸ: {timeframe}, çª—å£: {chunk_days} å¤©/æ¬¡, å“ç§: {symbol}")

        end_dt = datetime.now().date()
        start_dt = (datetime.now() - timedelta(days=365)).date()

        cursor = start_dt
        total_inserted = 0
        batch = 0

        while cursor < end_dt:
            batch += 1
            window_start = cursor
            window_end = min(end_dt, cursor + timedelta(days=chunk_days))

            sd = window_start.strftime('%Y-%m-%d')
            ed = window_end.strftime('%Y-%m-%d')

            print(f"\n[{batch}] ğŸ“¥ ä¸‹è½½åŒºé—´: {sd} ~ {ed}")
            df = self.get_forex_data(symbol, timeframe, start_date=sd, end_date=ed)
            if not df.empty:
                before = len(df)
                self.save_to_database(df, symbol, timeframe)
                total_inserted += before
                print(f"   âœ… æœ¬æ‰¹æ•°æ®: {before} æ¡ï¼Œç´¯è®¡: {total_inserted}")
            else:
                print("   âš ï¸ æœ¬æ‰¹æ— æ•°æ®ï¼ˆå¯èƒ½ä¸ºå‘¨æœ«/èŠ‚å‡æ—¥/æ•°æ®ç¼ºå¤±ï¼‰")

            # ç§»åŠ¨çª—å£
            cursor = window_end

        print("\nğŸ‰ åˆ†æ®µä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“Š ç´¯è®¡å¤„ç†è®°å½•ï¼ˆå«å»é‡å‰è®¡æ•°ï¼‰: {total_inserted:,}")
        return {"batches": batch, "total_rows": total_inserted}
    
    def get_data_by_timeframe(self, 
                             symbol: str, 
                             timeframe: str, 
                             limit: int = 200) -> pd.DataFrame:
        """
        æŒ‰æ—¶é—´å‘¨æœŸæŸ¥è¯¢å·²å­˜å‚¨çš„æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å“ç§
            timeframe: æ—¶é—´å‘¨æœŸ
            limit: æ•°æ®æ¡æ•°é™åˆ¶
        
        Returns:
            pandas.DataFrame: Kçº¿æ•°æ®
        """
        return self.load_from_database(symbol, timeframe, limit)
    
    def get_available_data(self) -> Dict:
        """
        æŸ¥çœ‹æ•°æ®åº“ä¸­å·²å­˜å‚¨çš„æ•°æ®æ¦‚å†µ
        
        Returns:
            Dict: æ•°æ®æ¦‚å†µç»Ÿè®¡
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                # æŒ‰å“ç§å’Œæ—¶é—´å‘¨æœŸç»Ÿè®¡
                cursor = conn.execute("""
                    SELECT symbol, timeframe, COUNT(*) as count, 
                           MIN(datetime) as earliest, MAX(datetime) as latest
                    FROM price_data
                    GROUP BY symbol, timeframe
                    ORDER BY symbol, timeframe
                """)
                
                data_summary = {}
                for symbol, timeframe, count, earliest, latest in cursor.fetchall():
                    if symbol not in data_summary:
                        data_summary[symbol] = {}
                    
                    data_summary[symbol][timeframe] = {
                        'count': count,
                        'earliest': earliest,
                        'latest': latest
                    }
                
                # æ€»ä½“ç»Ÿè®¡
                cursor = conn.execute("SELECT COUNT(*) FROM price_data")
                total_records = cursor.fetchone()[0]
                
                return {
                    'total_records': total_records,
                    'data_by_symbol_timeframe': data_summary
                }
                
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®æ¦‚å†µå¤±è´¥: {e}")
            return {'total_records': 0, 'data_by_symbol_timeframe': {}}

def download_data_by_date_range(client, symbol, timeframe, start_date, end_date):
    """æŒ‰æ—¥æœŸèŒƒå›´ä¸‹è½½æ•°æ®"""
    print(f"ğŸ—“ï¸  æŒ‰æ—¥æœŸèŒƒå›´ä¸‹è½½ {symbol} {timeframe} æ•°æ®")
    print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    print("-" * 50)
    
    try:
        df = client.get_forex_data(symbol, timeframe, start_date=start_date, end_date=end_date)
        
        if not df.empty:
            # ä¿å­˜åˆ°æ•°æ®åº“
            client.save_to_database(df, symbol, timeframe)
            
            # ç»Ÿè®¡ä¿¡æ¯
            actual_start = df['datetime'].min()
            actual_end = df['datetime'].max()
            latest_price = df['close'].iloc[-1]
            
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(df)} æ ¹Kçº¿")
            print(f"ğŸ“… å®é™…æ—¶é—´èŒƒå›´: {actual_start} ~ {actual_end}")
            print(f"ğŸ’° æœ€æ–°ä»·æ ¼: {latest_price:.5f}")
            
            return df
        else:
            print(f"âŒ æœªè·å–åˆ°æ•°æ®")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return pd.DataFrame()

def download_data_by_year(client, symbol, timeframe, year):
    """æŒ‰å¹´ä»½ä¸‹è½½æ•°æ®"""
    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"
    return download_data_by_date_range(client, symbol, timeframe, start_date, end_date)

def download_data_batch(client, symbol, timeframes, size):
    """æ‰¹é‡ä¸‹è½½å¤šæ—¶é—´å‘¨æœŸæ•°æ®"""
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ {symbol} æ•°æ®")
    print(f"ğŸ“‹ æ—¶é—´å‘¨æœŸ: {', '.join(timeframes)}")
    print(f"ğŸ“Š æ¯å‘¨æœŸæ•°é‡: {size} æ ¹Kçº¿")
    print("-" * 50)
    
    total_downloaded = 0
    successful_downloads = []
    failed_downloads = []
    
    for i, timeframe in enumerate(timeframes, 1):
        print(f"\n[{i}/{len(timeframes)}] ğŸ“¥ ä¸‹è½½ {timeframe} æ•°æ®...")
        
        try:
            df = client.get_forex_data(symbol, timeframe, size)
            
            if not df.empty:
                # ä¿å­˜åˆ°æ•°æ®åº“
                client.save_to_database(df, symbol, timeframe)
                
                # ç»Ÿè®¡ä¿¡æ¯
                date_range = f"{df['datetime'].min()} ~ {df['datetime'].max()}"
                latest_price = df['close'].iloc[-1]
                
                print(f"âœ… {timeframe}: {len(df)} æ ¹Kçº¿")
                print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {date_range}")
                print(f"   ğŸ’° æœ€æ–°ä»·æ ¼: {latest_price:.5f}")
                
                total_downloaded += len(df)
                successful_downloads.append({
                    'timeframe': timeframe,
                    'count': len(df),
                    'range': date_range
                })
            else:
                print(f"âŒ {timeframe}: æœªè·å–åˆ°æ•°æ®")
                failed_downloads.append(timeframe)
                
        except Exception as e:
            print(f"âŒ {timeframe}: ä¸‹è½½å¤±è´¥ - {e}")
            failed_downloads.append(timeframe)
    
    # ä¸‹è½½æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“ˆ ä¸‹è½½æ€»ç»“:")
    print(f"âœ… æˆåŠŸ: {len(successful_downloads)} ä¸ªæ—¶é—´å‘¨æœŸ")
    print(f"âŒ å¤±è´¥: {len(failed_downloads)} ä¸ªæ—¶é—´å‘¨æœŸ")
    print(f"ğŸ“Š æ€»è®¡ä¸‹è½½: {total_downloaded:,} æ ¹Kçº¿")
    
    if failed_downloads:
        print(f"âš ï¸  å¤±è´¥çš„æ—¶é—´å‘¨æœŸ: {', '.join(failed_downloads)}")
    
    return successful_downloads, failed_downloads

def analyze_data_quality(client, symbol="EUR/USD", timeframe="15min", min_records=70):
    """
    åˆ†ææ•°æ®è´¨é‡ï¼Œè¯†åˆ«éœ€è¦ä¿®å¤çš„æ—¥æœŸ
    
    Args:
        client: TwelveDataClientå®ä¾‹
        symbol: äº¤æ˜“å“ç§
        timeframe: æ—¶é—´å‘¨æœŸ
        min_records: æœ€å°‘è®°å½•æ•°é˜ˆå€¼
        
    Returns:
        Dict: åŒ…å«ä¸¥é‡ç¼ºå¤±å’Œä¸­ç­‰ç¼ºå¤±æ—¥æœŸçš„å­—å…¸
    """
    print(f"ğŸ“Š åˆ†æ {symbol} {timeframe} æ•°æ®è´¨é‡...")
    
    try:
        import sqlite3
        import pandas as pd
        
        with sqlite3.connect(client.db_path) as conn:
            # æŸ¥è¯¢ä½äºé˜ˆå€¼çš„å·¥ä½œæ—¥
            query = """
                SELECT 
                  DATE(datetime) as date,
                  strftime('%w', datetime) as weekday,
                  COUNT(*) as record_count
                FROM price_data 
                WHERE symbol=? AND timeframe=?
                  AND strftime('%w', datetime) NOT IN ('0','6')  -- æ’é™¤å‘¨æœ«
                GROUP BY DATE(datetime)
                HAVING COUNT(*) < ?
                ORDER BY record_count ASC, date;
            """
            
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, min_records))
            
            if df.empty:
                print(f"âœ… æ‰€æœ‰å·¥ä½œæ—¥çš„æ•°æ®è´¨é‡å‡>={min_records}è®°å½•ï¼Œæ— éœ€ä¿®å¤!")
                return {"severe": [], "moderate": [], "total": 0}
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
            severe_df = df[df['record_count'] < 40]  # ä¸¥é‡ç¼ºå¤±
            moderate_df = df[(df['record_count'] >= 40) & (df['record_count'] < min_records)]  # ä¸­ç­‰ç¼ºå¤±
            
            severe_dates = severe_df['date'].tolist()
            moderate_dates = moderate_df['date'].tolist()
            
            print(f"å‘ç° {len(df)} ä¸ªå·¥ä½œæ—¥æ•°æ®è´¨é‡éœ€è¦æ”¹å–„")
            print(f"ä¸¥é‡ç¼ºå¤± (<40è®°å½•): {len(severe_dates)} å¤©")
            print(f"ä¸­ç­‰ç¼ºå¤± (40-{min_records}è®°å½•): {len(moderate_dates)} å¤©")
            
            return {
                "severe": severe_dates,
                "moderate": moderate_dates,
                "total": len(df),
                "severe_details": severe_df.to_dict('records'),
                "moderate_details": moderate_df.to_dict('records')
            }
            
    except Exception as e:
        print(f"âŒ æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}")
        return {"severe": [], "moderate": [], "total": 0}

def fix_missing_data_batch(client, problem_dates, symbol="EUR/USD", timeframe="15min"):
    """
    æ‰¹é‡ä¿®å¤ç¼ºå¤±æ•°æ®
    
    Args:
        client: TwelveDataClientå®ä¾‹  
        problem_dates: é—®é¢˜æ—¥æœŸåˆ—è¡¨
        symbol: äº¤æ˜“å“ç§
        timeframe: æ—¶é—´å‘¨æœŸ
        
    Returns:
        Dict: ä¿®å¤ç»“æœç»Ÿè®¡
    """
    print(f"ğŸ”§ å¼€å§‹æ‰¹é‡ä¿®å¤ {len(problem_dates)} ä¸ªæ—¥æœŸçš„æ•°æ®...")
    print("=" * 60)
    
    from datetime import datetime, timedelta
    import time
    
    success_count = 0
    fail_count = 0
    total_added = 0
    
    for i, date_str in enumerate(problem_dates, 1):
        print(f"\n[{i}/{len(problem_dates)}] ä¿®å¤ {date_str}...")
        
        try:
            # åˆ›å»ºæ—¶é—´çª—å£ï¼šå‰åå„1å¤©
            target_date = datetime.strptime(date_str, '%Y-%m-%d')
            start_date = (target_date - timedelta(days=1)).strftime('%Y-%m-%d')
            end_date = (target_date + timedelta(days=1)).strftime('%Y-%m-%d')
            
            print(f"   ğŸ“… ä¸‹è½½æ—¶é—´çª—å£: {start_date} ~ {end_date}")
            
            # è·å–æ•°æ®
            df = client.get_forex_data(symbol, timeframe, start_date=start_date, end_date=end_date)
            
            if not df.empty:
                # ä¿å­˜æ•°æ®
                client.save_to_database(df, symbol, timeframe)
                
                # éªŒè¯ä¿®å¤æ•ˆæœï¼šæ£€æŸ¥ç›®æ ‡æ—¥æœŸçš„è®°å½•æ•°
                import sqlite3
                with sqlite3.connect(client.db_path) as conn:
                    cursor = conn.execute("""
                        SELECT COUNT(*) FROM price_data 
                        WHERE symbol=? AND timeframe=? AND DATE(datetime)=?
                    """, (symbol, timeframe, date_str))
                    new_count = cursor.fetchone()[0]
                
                print(f"   âœ… ä¿®å¤å®Œæˆï¼Œç›®æ ‡æ—¥æœŸç°æœ‰ {new_count} è®°å½•")
                success_count += 1
                total_added += len(df)
            else:
                print(f"   âŒ æœªè·å–åˆ°æ•°æ®")
                fail_count += 1
                
        except Exception as e:
            print(f"   âŒ ä¿®å¤å¤±è´¥: {e}")
            fail_count += 1
        
        # APIé™åˆ¶ï¼šæ¯7æ¬¡è°ƒç”¨åæš‚åœ60ç§’ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        if i % 7 == 0 and i < len(problem_dates):
            print(f"\nâ³ APIé™åˆ¶ä¿æŠ¤ï¼Œæš‚åœ60ç§’... ({i}/{len(problem_dates)} å·²å¤„ç†)")
            time.sleep(60)
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ‰ æ‰¹é‡ä¿®å¤å®Œæˆ!")
    print(f"âœ… æˆåŠŸä¿®å¤: {success_count} ä¸ªæ—¥æœŸ")
    print(f"âŒ ä¿®å¤å¤±è´¥: {fail_count} ä¸ªæ—¥æœŸ") 
    print(f"ğŸ“Š æ€»è®¡æ·»åŠ : {total_added:,} æ¡è®°å½•")
    
    return {
        "success": success_count,
        "failed": fail_count,
        "total_added": total_added
    }

def validate_fix_results(client, symbol="EUR/USD", timeframe="15min", min_records=70):
    """
    éªŒè¯ä¿®å¤ç»“æœ
    
    Args:
        client: TwelveDataClientå®ä¾‹
        symbol: äº¤æ˜“å“ç§  
        timeframe: æ—¶é—´å‘¨æœŸ
        min_records: æœ€å°‘è®°å½•æ•°é˜ˆå€¼
        
    Returns:
        Dict: ä¿®å¤åçš„è´¨é‡ç»Ÿè®¡
    """
    print(f"ğŸ” éªŒè¯ä¿®å¤ç»“æœ...")
    
    # é‡æ–°åˆ†ææ•°æ®è´¨é‡
    quality_report = analyze_data_quality(client, symbol, timeframe, min_records)
    
    try:
        import sqlite3
        with sqlite3.connect(client.db_path) as conn:
            # ç»Ÿè®¡æ€»ä½“æ•°æ®è´¨é‡
            cursor = conn.execute("""
                SELECT COUNT(*) as total_days,
                       AVG(record_count) as avg_records,
                       MIN(record_count) as min_records,
                       MAX(record_count) as max_records
                FROM (
                    SELECT DATE(datetime) as date, COUNT(*) as record_count
                    FROM price_data 
                    WHERE symbol=? AND timeframe=?
                      AND strftime('%w', datetime) NOT IN ('0','6')
                    GROUP BY DATE(datetime)
                )
            """, (symbol, timeframe))
            
            stats = cursor.fetchone()
            
            # æŒ‰è´¨é‡ç­‰çº§åˆ†ç±»ç»Ÿè®¡
            cursor = conn.execute("""
                SELECT 
                    CASE 
                        WHEN record_count >= 85 THEN 'excellent'
                        WHEN record_count >= 70 THEN 'good'  
                        WHEN record_count >= 40 THEN 'fair'
                        ELSE 'poor'
                    END as quality_grade,
                    COUNT(*) as day_count
                FROM (
                    SELECT DATE(datetime) as date, COUNT(*) as record_count
                    FROM price_data 
                    WHERE symbol=? AND timeframe=?
                      AND strftime('%w', datetime) NOT IN ('0','6')
                    GROUP BY DATE(datetime)
                )
                GROUP BY quality_grade;
            """, (symbol, timeframe))
            
            quality_breakdown = {row[0]: row[1] for row in cursor.fetchall()}
            
            print(f"\nğŸ“Š ä¿®å¤åæ•°æ®è´¨é‡æŠ¥å‘Š:")
            print(f"æ€»å·¥ä½œæ—¥æ•°: {stats[0]:,}")
            print(f"å¹³å‡è®°å½•æ•°: {stats[1]:.1f}")
            print(f"è®°å½•æ•°èŒƒå›´: {stats[2]} - {stats[3]}")
            print(f"\nè´¨é‡ç­‰çº§åˆ†å¸ƒ:")
            print(f"  ä¼˜ç§€ (>=85è®°å½•): {quality_breakdown.get('excellent', 0):,} å¤©")
            print(f"  è‰¯å¥½ (70-84è®°å½•): {quality_breakdown.get('good', 0):,} å¤©")
            print(f"  ä¸€èˆ¬ (40-69è®°å½•): {quality_breakdown.get('fair', 0):,} å¤©")  
            print(f"  è¾ƒå·® (<40è®°å½•): {quality_breakdown.get('poor', 0):,} å¤©")
            
            # è®¡ç®—è´¨é‡ç™¾åˆ†æ¯”
            total_days = stats[0] or 1
            good_days = quality_breakdown.get('excellent', 0) + quality_breakdown.get('good', 0)
            quality_percentage = (good_days / total_days) * 100
            
            print(f"\nğŸ¯ æ•°æ®è´¨é‡è¾¾æ ‡ç‡: {quality_percentage:.1f}% (>=70è®°å½•)")
            
            return {
                "total_days": stats[0],
                "avg_records": stats[1],
                "quality_breakdown": quality_breakdown,
                "quality_percentage": quality_percentage,
                "remaining_problems": quality_report["total"]
            }
            
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return {}

def main():
    """é€šç”¨æ‰¹é‡Kçº¿æ•°æ®ä¸‹è½½å·¥å…·"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='å¤šæ—¶é—´å‘¨æœŸKçº¿æ•°æ®æ‰¹é‡ä¸‹è½½å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä¸‹è½½2000æ ¹æ—¥çº¿æ•°æ®
  python3 twelve_data_client.py --timeframe 1day --size 2000
  
  # æŒ‡å®šæ—¥æœŸèŒƒå›´ä¸‹è½½
  python3 twelve_data_client.py --timeframe 4h --start-date 2020-01-01 --end-date 2020-12-31
  
  # ä¸‹è½½æŒ‡å®šå¹´ä»½æ•°æ®
  python3 twelve_data_client.py --timeframe 4h --year 2020
  
  # ä»2020å¹´å¼€å§‹ä¸‹è½½åˆ°ç°åœ¨
  python3 twelve_data_client.py --timeframe 4h --from-2020
  
  # ä¸‹è½½å¤šä¸ªæ—¶é—´å‘¨æœŸï¼Œæ¯ä¸ª1000æ ¹Kçº¿
  python3 twelve_data_client.py --timeframes 15min,1h,4h,1day --size 1000
  
  # ä¸‹è½½æ‰€æœ‰æ”¯æŒçš„æ—¶é—´å‘¨æœŸï¼Œæœ€å¤§æ•°æ®é‡
  python3 twelve_data_client.py --all --size 5000
  
  # åªæ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
  python3 twelve_data_client.py --show-stats
  
  # åœ°æ¯¯å¼ä¿®å¤æ‰€æœ‰ç¼ºå¤±æ•°æ® (æ¨è)
  python3 twelve_data_client.py --fix-data
  
  # åªä¿®å¤ä¸¥é‡ç¼ºå¤±çš„æ•°æ® (<40è®°å½•)
  python3 twelve_data_client.py --fix-severe
  
  # åªéªŒè¯æ•°æ®è´¨é‡ï¼Œä¸ä¿®å¤
  python3 twelve_data_client.py --validate-only
  
  # è®¾ç½®ä¿®å¤é˜ˆå€¼ä¸º80è®°å½•
  python3 twelve_data_client.py --fix-data --min-records 80
  
  # ä½¿ç”¨è‡ªå®šä¹‰APIå¯†é’¥
  python3 twelve_data_client.py --api-key your-key --timeframe 1day --size 2000
        """
    )
    
    parser.add_argument('--api-key', default="8ae7370c143b4751a4c8d1b426f0dcb7",
                       help='Twelve Data APIå¯†é’¥ (å·²å†…ç½®é»˜è®¤å¯†é’¥)')
    parser.add_argument('--symbol', '-s', default='EUR/USD', 
                       help='äº¤æ˜“å“ç§ (é»˜è®¤: EUR/USD)')
    parser.add_argument('--timeframe', '-t', 
                       help='å•ä¸ªæ—¶é—´å‘¨æœŸ (1min, 5min, 15min, 1h, 4h, 1day, 1week)')
    parser.add_argument('--timeframes', '-T', 
                       help='å¤šä¸ªæ—¶é—´å‘¨æœŸï¼Œé€—å·åˆ†éš” (å¦‚: 1min,5min,15min,1h,4h,1day)')
    parser.add_argument('--all', '-a', action='store_true',
                       help='ä¸‹è½½æ‰€æœ‰æ”¯æŒçš„æ—¶é—´å‘¨æœŸ')
    parser.add_argument('--size', '-n', type=int, default=5000,
                       help='æ¯ä¸ªæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°é‡ (é»˜è®¤: 5000, æœ€å¤§: 5000)')
    parser.add_argument('--show-stats', action='store_true',
                       help='æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯')
    
    # å¿«æ·ä»»åŠ¡ï¼šæœ€è¿‘ä¸€å¹´ 1min åˆ†æ®µä¸‹è½½
    parser.add_argument('--last-year-chunked', action='store_true',
                       help='ä¸‹è½½æœ€è¿‘ä¸€å¹´æ•°æ®ï¼ˆå»ºè®®é…åˆ --timeframe 1minï¼ŒæŒ‰çª—å£åˆ†æ®µå…¥åº“ï¼‰')
    parser.add_argument('--chunk-days', type=float, default=3.5,
                       help='åˆ†æ®µçª—å£å¤©æ•°ï¼Œâ‰ˆ5000æ ¹/æ¬¡ï¼ˆé»˜è®¤: 3.0ï¼‰')
    
    # æ—¥æœŸç›¸å…³å‚æ•°
    parser.add_argument('--start-date', '-sd', 
                       help='å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--end-date', '-ed', 
                       help='ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--year', '-y', type=int,
                       help='æŒ‡å®šå¹´ä»½ä¸‹è½½ (å¦‚: 2020)')
    parser.add_argument('--from-2020', action='store_true',
                       help='ä»2020å¹´å¼€å§‹ä¸‹è½½åˆ°ç°åœ¨çš„æ‰€æœ‰æ•°æ®')
    
    # æ•°æ®ä¿®å¤ç›¸å…³å‚æ•°
    parser.add_argument('--fix-data', action='store_true',
                       help='ä¿®å¤ç¼ºå¤±æ•°æ®æ¨¡å¼')
    parser.add_argument('--fix-severe', action='store_true',
                       help='åªä¿®å¤ä¸¥é‡ç¼ºå¤±çš„æ•°æ® (<40è®°å½•)')
    parser.add_argument('--min-records', type=int, default=70,
                       help='æ•°æ®è´¨é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„å·¥ä½œæ—¥å°†è¢«ä¿®å¤ (é»˜è®¤: 70)')
    parser.add_argument('--validate-only', action='store_true',
                       help='åªéªŒè¯æ•°æ®è´¨é‡ï¼Œä¸è¿›è¡Œä¿®å¤')
    parser.add_argument('--auto-confirm', action='store_true',
                       help='è‡ªåŠ¨ç¡®è®¤ä¿®å¤æ“ä½œï¼Œæ— éœ€ç”¨æˆ·è¾“å…¥')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.size > 5000 or args.size < 1:
        print("âŒ é”™è¯¯: sizeå‚æ•°å¿…é¡»åœ¨1-5000èŒƒå›´å†…")
        return 1
    
    # éªŒè¯æ—¥æœŸå‚æ•°
    date_params_count = sum([bool(args.start_date), bool(args.year), bool(args.from_2020)])
    if date_params_count > 1:
        print("âŒ é”™è¯¯: ä¸èƒ½åŒæ—¶ä½¿ç”¨å¤šä¸ªæ—¥æœŸå‚æ•° (--start-date, --year, --from-2020)")
        return 1
    
    if args.start_date and not args.end_date:
        print("âŒ é”™è¯¯: ä½¿ç”¨ --start-date æ—¶å¿…é¡»åŒæ—¶æä¾› --end-date")
        return 1
    
    if args.end_date and not args.start_date:
        print("âŒ é”™è¯¯: ä½¿ç”¨ --end-date æ—¶å¿…é¡»åŒæ—¶æä¾› --start-date")
        return 1
        
    if args.start_date and not validate_date_format(args.start_date):
        print("âŒ é”™è¯¯: start-date æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        return 1
        
    if args.end_date and not validate_date_format(args.end_date):
        print("âŒ é”™è¯¯: end-date æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        return 1
    
    print("ğŸ¤– å¤šæ—¶é—´å‘¨æœŸKçº¿æ•°æ®æ‰¹é‡ä¸‹è½½å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    try:
        client = TwelveDataClient(args.api_key)
    except Exception as e:
        print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1
    
    # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
    if args.show_stats:
        print("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        overview = client.get_available_data()
        print(f"æ€»è®°å½•æ•°: {overview['total_records']:,}")
        
        if overview['data_by_symbol_timeframe']:
            for symbol, timeframes in overview['data_by_symbol_timeframe'].items():
                print(f"\nğŸ’± {symbol}:")
                for tf, info in timeframes.items():
                    print(f"  {tf: <8}: {info['count']:>6} æ¡è®°å½• ({info['earliest']} ~ {info['latest']})")
        else:
            print("ğŸ“ æ•°æ®åº“ä¸ºç©º")
        
        if not any([args.timeframe, args.timeframes, args.all, args.fix_data, args.fix_severe, args.validate_only]):
            return 0
    
    # æ•°æ®ä¿®å¤æ¨¡å¼
    if args.fix_data or args.fix_severe or args.validate_only:
        tf = args.timeframe or '1min'
        print(f"\nğŸ”§ æ•°æ®ä¿®å¤æ¨¡å¼: {args.symbol} {tf}")
        print("=" * 60)
        
        # æ•°æ®è´¨é‡åˆ†æ
        quality_report = analyze_data_quality(client, args.symbol, tf, args.min_records)
        
        if args.validate_only:
            # åªéªŒè¯æ¨¡å¼
            validate_fix_results(client, args.symbol, tf, args.min_records)
            return 0
        
        # ç¡®å®šè¦ä¿®å¤çš„æ—¥æœŸ
        if args.fix_severe:
            # åªä¿®å¤ä¸¥é‡ç¼ºå¤±
            problem_dates = quality_report["severe"]
            print(f"\nğŸš¨ ä¸¥é‡ç¼ºå¤±ä¿®å¤æ¨¡å¼: {len(problem_dates)} ä¸ªæ—¥æœŸ")
        else:
            # ä¿®å¤æ‰€æœ‰é—®é¢˜æ•°æ®
            problem_dates = quality_report["severe"] + quality_report["moderate"]
            print(f"\nğŸ”§ å…¨é¢ä¿®å¤æ¨¡å¼: {len(problem_dates)} ä¸ªæ—¥æœŸ")
        
        if not problem_dates:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤çš„æ•°æ®!")
            return 0
        
        # ç”¨æˆ·ç¡®è®¤
        print(f"\nâš ï¸  å°†è¦ä¿®å¤ {len(problem_dates)} ä¸ªå·¥ä½œæ—¥çš„æ•°æ®")
        print(f"é¢„è®¡APIè°ƒç”¨: {len(problem_dates)} æ¬¡")
        print(f"é¢„è®¡æ—¶é—´: {len(problem_dates) // 7 + 1} åˆ†é’Ÿ (æ¯7æ¬¡è°ƒç”¨æš‚åœ60ç§’)")
        
        if not args.auto_confirm:
            response = input("\næ˜¯å¦ç»§ç»­? (y/N): ").strip().lower()
            if response not in ['y', 'yes']:
                print("âŒ ç”¨æˆ·å–æ¶ˆä¿®å¤")
                return 0
        else:
            print("\nâœ… è‡ªåŠ¨ç¡®è®¤æ¨¡å¼ï¼Œå¼€å§‹ä¿®å¤...")
        
        # æ‰§è¡Œä¿®å¤
        fix_result = fix_missing_data_batch(client, problem_dates, args.symbol, tf)
        
        # éªŒè¯ä¿®å¤ç»“æœ
        print("\n" + "=" * 60)
        validate_fix_results(client, args.symbol, tf, args.min_records)
        
        return 0 if fix_result["failed"] == 0 else 1
    
    # ç¡®å®šè¦ä¸‹è½½çš„æ—¶é—´å‘¨æœŸ
    all_timeframes = ["1min", "5min", "15min", "30min", "1h", "2h", "4h", "1day", "1week"]
    
    if args.all:
        timeframes = all_timeframes
    elif args.timeframes:
        timeframes = [tf.strip() for tf in args.timeframes.split(',')]
        # éªŒè¯æ—¶é—´å‘¨æœŸæœ‰æ•ˆæ€§
        invalid_tfs = [tf for tf in timeframes if tf not in all_timeframes]
        if invalid_tfs:
            print(f"âŒ é”™è¯¯: æ— æ•ˆçš„æ—¶é—´å‘¨æœŸ: {', '.join(invalid_tfs)}")
            print(f"âœ… æ”¯æŒçš„æ—¶é—´å‘¨æœŸ: {', '.join(all_timeframes)}")
            return 1
    elif args.timeframe:
        if args.timeframe not in all_timeframes:
            print(f"âŒ é”™è¯¯: æ— æ•ˆçš„æ—¶é—´å‘¨æœŸ: {args.timeframe}")
            print(f"âœ… æ”¯æŒçš„æ—¶é—´å‘¨æœŸ: {', '.join(all_timeframes)}")
            return 1
        timeframes = [args.timeframe]
    else:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¦ä¸‹è½½çš„æ—¶é—´å‘¨æœŸ (--timeframe, --timeframes, æˆ– --all)")
        return 1
    
    # ç‰¹æ®Šå¿«æ·ä»»åŠ¡ï¼šæœ€è¿‘ä¸€å¹´åˆ†æ®µä¸‹è½½
    if args.last_year_chunked:
        tf = args.timeframe or '1min'
        if tf != '1min':
            print(f"âš ï¸ æç¤º: å½“å‰è®¾ç½® timeframe={tf}ï¼Œè¯¥ä»»åŠ¡ä¸»è¦é¢å‘ 1min å‘¨æœŸã€‚")
        try:
            client.download_last_year_in_chunks(symbol=args.symbol, timeframe=tf, chunk_days=args.chunk_days)
            return 0
        except Exception as e:
            print(f"âŒ åˆ†æ®µä¸‹è½½å¤±è´¥: {e}")
            return 1

    # æ‰§è¡Œä¸‹è½½
    try:
        successful = []
        failed = []
        
        # æ ¹æ®æ—¥æœŸå‚æ•°é€‰æ‹©ä¸‹è½½ç­–ç•¥
        if args.start_date and args.end_date:
            # æ—¥æœŸèŒƒå›´ä¸‹è½½
            for timeframe in timeframes:
                result_df = download_data_by_date_range(client, args.symbol, timeframe, args.start_date, args.end_date)
                if not result_df.empty:
                    successful.append(timeframe)
                else:
                    failed.append(timeframe)
        elif args.year:
            # å¹´ä»½ä¸‹è½½
            for timeframe in timeframes:
                result_df = download_data_by_year(client, args.symbol, timeframe, args.year)
                if not result_df.empty:
                    successful.append(timeframe)
                else:
                    failed.append(timeframe)
        elif args.from_2020:
            # ä»2020å¹´å¼€å§‹åˆ†å¹´ä¸‹è½½
            current_year = datetime.now().year
            for timeframe in timeframes:
                print(f"\nğŸ—“ï¸  ä¸‹è½½ {timeframe} ä»2020å¹´åˆ°ç°åœ¨çš„æ‰€æœ‰æ•°æ®")
                year_success = 0
                for year in range(2020, current_year + 1):
                    print(f"  ğŸ“… ä¸‹è½½ {year} å¹´æ•°æ®...")
                    result_df = download_data_by_year(client, args.symbol, timeframe, year)
                    if not result_df.empty:
                        year_success += 1
                if year_success > 0:
                    successful.append(f"{timeframe}({year_success}å¹´)")
                else:
                    failed.append(timeframe)
        else:
            # ä¼ ç»Ÿæ‰¹é‡ä¸‹è½½
            successful, failed = download_data_batch(client, args.symbol, timeframes, args.size)
        
        # æœ€ç»ˆç»Ÿè®¡
        if args.show_stats and (successful or failed):
            print("\nğŸ“ˆ æ›´æ–°åæ•°æ®åº“ç»Ÿè®¡:")
            final_overview = client.get_available_data()
            print(f"æ€»è®°å½•æ•°: {final_overview['total_records']:,}")
        
        print("\nğŸ‰ ä¸‹è½½å®Œæˆ!")
        if successful:
            print(f"âœ… æˆåŠŸ: {len(successful)} é¡¹")
        if failed:
            print(f"âŒ å¤±è´¥: {len(failed)} é¡¹")
        return 0 if not failed else 1
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        return 1
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())
